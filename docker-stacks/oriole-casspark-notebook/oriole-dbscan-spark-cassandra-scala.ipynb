{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection and pattern extraction with Spark, Cassandra and Scala\n",
    "\n",
    "Today, geo-located data is available in a number of domains, ranging from healthcare to financial markets, to social services. In all these domains, extracting patterns and detecting anomalies and novelties from data has very concrete business outcomes. \n",
    "\n",
    "Anomaly detection can be defined as the process of finding which samples in the given dataset do not follow the given patterns and behave as though they were produced by a different mechanism. From detection follows action. Depending on the domain and the use case, we define them as anomalies or novelties and these signals are the triggers for applications such as personalized marketing and fraud alerting and notification.\n",
    "\n",
    "As more data gets ingested/produced via digital services, itâ€™s key to perform this sort of analytics at scale. In the open source space, technologies such as Spark and Cassandra are definitely instrumental to implement and execute modern data pipelines at scale.\n",
    "\n",
    "This Oriole is divided in two parts: Firstly, I will show how to collect data from Cassandra and bring it up to Spark for further analysis. In the second part of this Oriole, I will explore a number of techniques for detecting anomalies based on three different techniques:\n",
    "\n",
    "  - Statistics and Histograms\n",
    "  - Process Mining and Graph Analytics\n",
    "  - Clustering for Geo-Located Data with DBSCAN\n",
    "\n",
    "For this analysis, we are going to use the Gowalla Dataset [1]. The Gowalla dataset consists of a table of events, registered by anonymized users. Each event registers a user checking into a geolocated venue at a specific timestamp. The dataset is available at https://snap.stanford.edu/data/loc-gowalla.html\n",
    "\n",
    "[1] E. Cho, S. A. Myers, J. Leskovec. Friendship and Mobility: Friendship and Mobility: User Movement in Location-Based Social Networks ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "This notebook is running scala code and interfaces to a Spark cluster using the [Apache Toree](https://toree.incubator.apache.org/) project. Furthermore, Spark reads the data from Cassandra tables. Spark interfaces to Cassandra via the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). \n",
    "\n",
    "At the time of compiling this notebook, Spark 1.6.1 and Cassandra 3.5 were used. Here below the command to install the Spark - Scala Kernel on Jupiter. More instructions on this topic are available on Apache Toree [website](https://toree.incubator.apache.org/) and [github pages](https://github.com/apache/incubator-toree).\n",
    "\n",
    "```\n",
    "sudo jupyter-toree install --spark_home=${SPARK_HOME} \n",
    "--spark_opts='--packages com.datastax.spark:spark-cassandra\n",
    "-connector_2.10:1.6.0,graphframes:graphframes:0.1.0-spark1.6 \n",
    "--conf spark.cassandra.connection.host=localhost --conf \n",
    "spark.executor.memory=4g --conf spark.driver.memory=4g'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scala version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//sql context\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val sqlContext  = new SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// spark-cassandra connector\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._\n",
    "\n",
    "import org.apache.spark.sql.cassandra.CassandraSQLContext\n",
    "val cc = new CassandraSQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL queries in Cassandra\n",
    "\n",
    "Cassandra is exposed via a SQL context, so there is not need to learn a separate syntax as Spark will map the query to the available features of the underlying storage system. See below a simple query accessing the name and the id of venues from a cassandra table. Also remember that sql statements are _staged_ but not _executed_ until some actual [actions](http://spark.apache.org/docs/latest/programming-guide.html#actions) needs to be computed. Examples of actions are for instance, **count**(), **first**(), **collect**()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val venues   = cc.sql(\"select vid, name from lbsn.venues\").as(\"venues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30366"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1073929,Sputnik Gallery]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you cannot push to cassandra the full query, as the full query cannot be mapped on the available database functions supported on that specific data store. For instance, Cassandra cannot easily execute joins. In this case, Spark will partition and plan the query _pushing down_ what can be done in Cassandra and perform in Spark the rest of the query. \n",
    "\n",
    "More information can be found on Cassandra Documentation about [using Spark SQL to query data](http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkSqlOverview.html) or on the [Cassandra Spark Connector](https://github.com/datastax/spark-cassandra-connector) pages.\n",
    "\n",
    "For example, the following query filters out only those events which were registered in the New York area. As filtering in cassandra cannot by done on columns which are not indexes, this specific query will first move the data form Cassandra to Spark, and then will perform the filtering in Spark. In general, it's a good practice to push down and filter as much data as early as possible. This practice keeps the throughput low and minimize the data transfered from one system to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val events   = cc.sql(\"\"\"select ts, uid, lat, lon, vid from lbsn.events where\n",
    "                            lon>-74.2589 and lon<-73.7004 and \n",
    "                            lat> 40.4774 and lat< 40.9176\n",
    "                      \"\"\").as(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138948"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009-12-17 14:58:23.0,83942,40.7586191119,-73.9888966084,225371]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into anomaly detection of geo-located data, let's perform some more basic queries. Herebelow, it is shown how to count events registered by user uid=0, and how to retrieve the name of venue id 7239827. Finally, the third query prints out the first five rows of the venue table, in no particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// User 0: how many checkins?\n",
    "events.where(\"uid=0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7239827,Central Park Manhattan ]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.where(\"vid = 7239827\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|vid    |name                |\n",
      "+-------+--------------------+\n",
      "|1073929|Sputnik Gallery     |\n",
      "|1350193|Kaori's Closet Tokyo|\n",
      "|1425555|Club Spain          |\n",
      "|7160165|La Quinta - Brooklyn|\n",
      "|285750 |La Sorrentina       |\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "venues.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Cassandra tables in Spark.\n",
    "\n",
    "One of the advantages of connecting Cassandra and Spark, is the fact that you can now merge and join Cassandra tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid    |name                |\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "|2009-12-17 14:58:23.0|83942 |40.7586191119|-73.9888966084|225371 |Smith's Bar & Grill |\n",
      "|2010-08-18 03:08:36.0|166583|40.697517133 |-74.175481333 |1276055|Jake's Coffeehouse  |\n",
      "|2010-09-14 14:15:22.0|41568 |40.7538465143|-73.9845085144|1308099|The Southwest Porch |\n",
      "|2009-12-04 16:24:03.0|74122 |40.721437025 |-73.9978015423|15998  |La Esquina          |\n",
      "|2009-12-04 16:23:01.0|74122 |40.7211204821|-73.9982306578|32674  |Blip.tv Headquarters|\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_ny = events.\n",
    "  join(venues, events(\"events.vid\") === venues(\"venues.vid\"), \"inner\").\n",
    "  select(\"ts\", \"uid\", \"lat\", \"lon\", \"events.vid\",\"venues.name\")\n",
    "\n",
    "df_ny.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spark table joined above is going to be the starting point for our anomaly analysis.    \n",
    "Each row records the event's timestamp, the user id, the geo-location (latitude and longitude) of venue and finally the venue id and name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing SQL statements as code.\n",
    "\n",
    "Spark dataframes can also be filtered and transformed programmatically via a number of [pre-defined functions](https://spark.apache.org/docs/1.6.1/api/scala/#org.apache.spark.sql.functions$), such as min, sum, stddev, and many more. Some of those are shown in the next code sections. \n",
    "\n",
    "Next to the default set of pre-defined dataframe and column functions, it is possible to define the user-defined-functions (udf's). In the code below, we will create two UDF's to transform the timestamp to the day of the week and the hour of the day values, computed according to a given local timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// UDF functions for SQL-like operations on columns\n",
    "import org.joda.time.DateTime\n",
    "import org.joda.time.DateTimeZone\n",
    "\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val  dayofweek = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  // sunday starts at 0\n",
    "  dt.getDayOfWeek() - 1\n",
    "})\n",
    "\n",
    "val  localhour = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  dt.getHourOfDay()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "|ts                   |uid   |lat          |lon           |vid    |name                |dow|hour|\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "|2009-12-17 14:58:23.0|83942 |40.7586191119|-73.9888966084|225371 |Smith's Bar & Grill |3  |17  |\n",
      "|2010-08-18 03:08:36.0|166583|40.697517133 |-74.175481333 |1276055|Jake's Coffeehouse  |2  |6   |\n",
      "|2010-09-14 14:15:22.0|41568 |40.7538465143|-73.9845085144|1308099|The Southwest Porch |1  |17  |\n",
      "|2009-12-04 16:24:03.0|74122 |40.721437025 |-73.9978015423|15998  |La Esquina          |4  |19  |\n",
      "|2009-12-04 16:23:01.0|74122 |40.7211204821|-73.9982306578|32674  |Blip.tv Headquarters|4  |19  |\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val newyork_tz = \"America/New_York\"\n",
    "\n",
    "val df = df_ny.\n",
    "  withColumn(\"dow\",  dayofweek($\"ts\", lit(newyork_tz))).\n",
    "  withColumn(\"hour\", localhour($\"ts\", lit(newyork_tz))).\n",
    "  as(\"events\").cache()\n",
    "  \n",
    "df.show(5, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "### Histogram based\n",
    "#### Basic statistics in Spark\n",
    "\n",
    "The following code section shows how to collect global statistics and histograms per hour of the day and per day of the week. Histograms can be made more specific by aggregating the events according to a number of factors, such as:\n",
    "\n",
    " - venue\n",
    " - geographical area\n",
    " - popular users\n",
    " - 1st, 2nd friend's circle\n",
    " \n",
    "If you are interested, in multiple slicing and dicing option, Spark as a [cube function](https://spark.apache.org/docs/1.5.1/api/scala/index.html#org.apache.spark.sql.DataFrame) as well.  \n",
    "To start, let's compute the histogram, accumulating all events and aggregating by hour of the day and by day of the week.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dow|count|\n",
      "+---+-----+\n",
      "|  0|14640|\n",
      "|  1|15385|\n",
      "|  2|15686|\n",
      "|  3|16179|\n",
      "|  4|16558|\n",
      "|  5|18085|\n",
      "|  6|15849|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram day of the week events\n",
    "df.groupBy($\"dow\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|0   |2681 |\n",
      "|1   |1272 |\n",
      "|2   |852  |\n",
      "|3   |554  |\n",
      "|4   |323  |\n",
      "|5   |550  |\n",
      "|6   |937  |\n",
      "|7   |2157 |\n",
      "|8   |3615 |\n",
      "|9   |4586 |\n",
      "|10  |4903 |\n",
      "|11  |5576 |\n",
      "|12  |7257 |\n",
      "|13  |8142 |\n",
      "|14  |7687 |\n",
      "|15  |7254 |\n",
      "|16  |7519 |\n",
      "|17  |7495 |\n",
      "|18  |8621 |\n",
      "|19  |8970 |\n",
      "|20  |7167 |\n",
      "|21  |6127 |\n",
      "|22  |4747 |\n",
      "|23  |3390 |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram hour of the day events\n",
    "df.groupBy($\"hour\").count().show(24,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics in Spark: venue-specific histograms\n",
    "\n",
    "Moving on, let's have a look on how to create an specific histogram for each venue.   \n",
    "In this case, we will store the histogram as a vector. First, let's convert the day-of-the-week to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.linalg.DenseVector\n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector,Vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// vector histogram: the RDD way\n",
    "\n",
    "def toVector(i: Int, length:Int) = {\n",
    "  DenseVector((0 to length-1).map(x => if (x == i) 1.0 else 0.0).toArray)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225371,DenseVector(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select($\"vid\", $\"dow\").map(r => (r.getLong(0),toVector(r.getInt(1), 7))).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair RDDs: reduce by Key\n",
    "\n",
    "We will now **reduceByKey** those weekly and daily vectors by applying vectors arithmetics. In this way, we can collect the probability of an event happening at a specific day of the week for each venue in the dataset. This is a much more detailed analysis since different venues, such as restaurants, musea and train stations have different daily and weekly histogram patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dow_hist = df.\n",
    "  select($\"vid\", $\"dow\").\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 7))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray)).\n",
    "  toDF(\"vid\", \"dow_hist\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|vid    |dow_hist                       |\n",
      "+-------+-------------------------------+\n",
      "|713995 |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|4415530|[0.0,0.0,0.0,0.0,0.0,1.0,0.0]  |\n",
      "|486265 |[0.25,0.0,0.0,0.0,0.0,0.5,0.25]|\n",
      "+-------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dow_hist.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring events according to venues histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid    |dow|dow_hist                       |\n",
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "|2010-03-15 04:45:53.0|155999|40.7131656401|-74.0353098807|713995 |0  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-06-19 13:08:21.0|41235 |40.7131656401|-74.0353098807|713995 |5  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-04-23 20:47:34.0|10687 |40.7131656401|-74.0353098807|713995 |4  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-10-08 04:53:51.0|74222 |40.7131656401|-74.0353098807|713995 |4  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-09-25 10:02:38.0|181811|40.76603735  |-73.78952235  |4415530|5  |[0.0,0.0,0.0,0.0,0.0,1.0,0.0]  |\n",
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  select(\"ts\", \"uid\", \"lat\", \"lon\", \"events.vid\", \"dow\", \"dow_hist\").\n",
    "  cache()\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram based anomaly detection\n",
    "We will now score the probability of a given event based on the histograms we have just computed here above. For that, we are going to craft a new UDF which will select a given element of a vector based on the value provided by a different column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+------+---+--------+\n",
      "|ts                   |uid   |vid   |dow|dow_prob|\n",
      "+---------------------+------+------+---+--------+\n",
      "|2010-03-15 04:45:53.0|155999|713995|0  |0.25    |\n",
      "|2010-06-19 13:08:21.0|41235 |713995|5  |0.25    |\n",
      "|2010-04-23 20:47:34.0|10687 |713995|4  |0.5     |\n",
      "+---------------------+------+------+---+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val  nth = udf( (i:Int, arr: Vector) => {\n",
    "  arr.toArray.lift(i).getOrElse(0.0)\n",
    "})\n",
    "\n",
    "df_probs.select($\"ts\", $\"uid\", $\"vid\", $\"dow\", nth($\"dow\", $\"dow_hist\").as(\"dow_prob\")).show(3,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms for anomaly detection: putting it all together\n",
    "\n",
    "Let's repeat the same exercise for the histograms binned by hour of the day. And finally, let's merge and compute the probability of each given event, given the venue, the hour of the day, and the day of the week. Events with lower probabilities are less likely to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "|vid    |hour_hist                                                                                          |\n",
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "|713995 |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.25]|\n",
      "|4415530|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\n",
      "|486265 |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.25,0.0,0.25,0.0,0.0,0.0,0.0]|\n",
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// same for hour of the day\n",
    "\n",
    "val hour_hist = df.\n",
    "  select($\"vid\", $\"hour\").\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 24))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray)).\n",
    "  toDF(\"vid\", \"hour_hist\")\n",
    "\n",
    "hour_hist.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid  |hour_prob           |dow_prob           |\n",
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "|2010-08-13 23:28:29.0|125327|40.7490532543|-73.9680397511|11831|0.015873015873015872|0.19047619047619047|\n",
      "|2010-09-29 12:08:36.0|578   |40.7490532543|-73.9680397511|11831|0.12698412698412698 |0.2222222222222222 |\n",
      "|2010-09-05 13:20:27.0|578   |40.7490532543|-73.9680397511|11831|0.047619047619047616|0.12698412698412698|\n",
      "|2010-07-21 07:02:07.0|578   |40.7490532543|-73.9680397511|11831|0.07936507936507936 |0.2222222222222222 |\n",
      "|2010-07-20 14:24:06.0|578   |40.7490532543|-73.9680397511|11831|0.07936507936507936 |0.14285714285714285|\n",
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  join(hour_hist, df(\"vid\") === hour_hist(\"vid\"), \"inner\").\n",
    "  select( \n",
    "    $\"ts\", \n",
    "    $\"uid\", \n",
    "    $\"lat\", \n",
    "    $\"lon\", \n",
    "    $\"events.vid\", \n",
    "    nth($\"hour\", $\"hour_hist\").as(\"hour_prob\"), \n",
    "    nth($\"dow\",  $\"dow_hist\").as(\"dow_prob\")).\n",
    "  cache()\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process mining based\n",
    "\n",
    "The first step, in order to do process mining, is to collect sequences of events. In particular, the following code will take chronologically consecutive events and bundle them in pairs for a specific user. These pairs consists of two venue ids, namely source and destination, defining where each user is coming from, going to respectively. \n",
    "\n",
    "The steps in the following code are:\n",
    "\n",
    "  - Convert the DataFrame to an RDD\n",
    "  - Select uid as the key for the PairRDD\n",
    "  - Reshape the PairRDD from \"tall\" to \"wide\"\n",
    "  - Sort chronologically all the checked-in venues for each user\n",
    "  - Extract pairs from each sequence of checked-in venues per user\n",
    "  - Reshape the PairRDD from \"wide\" to \"tall\" again\n",
    "  - Convert back the PairRDD to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// process mining\n",
    "val g_df = events.\n",
    "  select($\"ts\", $\"uid\", $\"vid\").\n",
    "  rdd.\n",
    "  map(row => (row.getLong(1), List( (row.getTimestamp(0), row.getLong(2)) ))).\n",
    "  reduceByKey(_ ++ _).\n",
    "  mapValues( x =>\n",
    "    x.sortWith(_._1.getTime < _._1.getTime).\n",
    "      map(_._2)\n",
    "  ).\n",
    "  mapValues(_.sliding(2).toList).\n",
    "  flatMap(_._2).\n",
    "  map(\n",
    "    _ match {\n",
    "      case List(a, b) => Some((a, b))\n",
    "      case _ => None\n",
    "  }).\n",
    "  flatMap(x => x).\n",
    "  toDF(\"src\", \"dst\").\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newly created DataFrame is used to create a graph, where the nodes are the venues and the edges are connections of users checking-in from the one venue to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   src|   dst|\n",
      "+------+------+\n",
      "|255148|603177|\n",
      "|603177|603177|\n",
      "|603177|603177|\n",
      "|603177|603177|\n",
      "|603177|188022|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   src|   dst|\n",
      "+------+------+\n",
      "|886164|487279|\n",
      "| 12149|748034|\n",
      "|104364| 15079|\n",
      "|248066|853505|\n",
      "|655145|985182|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val edges_df = g_df.\n",
    "  groupBy($\"src\",$\"dst\").\n",
    "  count().\n",
    "  select($\"src\",$\"dst\").\n",
    "  filter($\"src\" !== $\"dst\").\n",
    "  cache()\n",
    "\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    id|\n",
      "+------+\n",
      "|220031|\n",
      "|105831|\n",
      "| 28031|\n",
      "| 11831|\n",
      "|434031|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val nodes_df = edges_df.\n",
    "  select($\"src\").\n",
    "  unionAll(edges_df.select($\"dst\")).\n",
    "  distinct().\n",
    "  toDF(\"id\").\n",
    "  cache()\n",
    "\n",
    "nodes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes = 21429\n",
      "# edges = 108757\n"
     ]
    }
   ],
   "source": [
    "println(s\"# nodes = ${nodes_df.count()}\")\n",
    "println(s\"# edges = ${edges_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph and Page Rank\n",
    "\n",
    "The above table describes how users are moving from venue to venue. We can now calculate which venues attract more users. This can be done using the page rank algorithm. The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly walking in the city will arrive at any particular venue. This analysis can be executed in Spark using [GraphFrames](http://graphframes.github.io/). GraphFrames is a package for Apache Spark which provides DataFrame-based graph analytics, including the PageRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.graphframes.GraphFrame\n",
    "\n",
    "val g = GraphFrame(nodes_df, edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = g.pageRank.resetProbability(0.05).maxIter(5).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------------------------+\n",
      "|vid   |pagerank          |name                             |\n",
      "+------+------------------+---------------------------------+\n",
      "|12505 |27.881643763277896|LGA LaGuardia Airport            |\n",
      "|23261 |26.385424367704715|JFK John F. Kennedy International|\n",
      "|11844 |22.11151212984308 |Times Square                     |\n",
      "|13022 |17.736644176952165|Grand Central Terminal           |\n",
      "|24963 |16.479439108529313|EWR Newark Liberty International |\n",
      "|11875 |10.202291677899952|Madison Square Garden            |\n",
      "|12525 |10.166984788526063|The Museum of Modern Art (MoMA)  |\n",
      "|11720 |10.0143847403669  |Yankee Stadium                   |\n",
      "|106840|9.76488986972341  |Union Square                     |\n",
      "|11834 |8.960647198372016 |Bryant Park                      |\n",
      "|12313 |8.15588197657451  |Empire State Building            |\n",
      "|14151 |6.999067399261288 |Rockefeller Center               |\n",
      "|17710 |6.986291334077661 |Madison Square Park              |\n",
      "+------+------------------+---------------------------------+\n",
      "only showing top 13 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val vertices = results.vertices.select(\"id\", \"pagerank\")\n",
    "val popular_venues = vertices.join(venues, vertices(\"id\") === venues(\"vid\"), \"inner\").select(\"vid\", \"pagerank\", \"name\")\n",
    "\n",
    "popular_venues.sort($\"pagerank\".desc).show(13, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this algorithm provides a \"popularity\" factor for each checked-in venue. This feature can be used to further discriminate anomalies based on the rank of the venue, for instance combining it with the probability of checking in a specific time of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-Location: density based\n",
    "\n",
    "We will now cluster events based on the [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN). DBSCAN is clustering events depending on the density of the events provided. Since the clusters emerge locally by looking for neighboring points, clusters of various shapes can be detected. Points that are isolated and too far from any other point are assigned to a special cluster of outliers. These discerning properties make the DBSCAN algorithm a good candidate for clustering geolocated events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://github.com/natalinobusa/nak/raw/master/nak_2.10-1.3.jar\n",
      "Finished download of nak_2.10-1.3.jar\n"
     ]
    }
   ],
   "source": [
    "%addjar https://github.com/natalinobusa/nak/raw/master/nak_2.10-1.3.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data by transforming the events DataFrame, into a PairRDD. In particular, for geolocated data, we choose the key to be the user identifier, and the value to be the aggregated list of all check-ins posted by that given user. The geolocated data is arranged in a n-by-2 matrix, where the first column represents the latitude and the second column the longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val e_df = events.\n",
    "  select(\"uid\",\"lat\",\"lon\").\n",
    "  rdd.map(row => (row.getLong(0), Array(row.getDouble(1), row.getDouble(2))) ).\n",
    "  reduceByKey( _ ++ _).\n",
    "  mapValues(v => new DenseMatrix(v.length/2,2,v, 0, 2, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatUserEvents(x: Tuple2[Long, DenseMatrix[Double]]) : Unit = {\n",
    "    val arr = x._2\n",
    "    val n = math.min( 5 , arr.rows) - 1\n",
    "    val slice = arr(0 to n, ::)\n",
    "    println(s\"uid = ${x._1}\")\n",
    "    println(s\"events count = ${arr.rows}\")\n",
    "    println(\"lat,lon = \")\n",
    "    println(slice)\n",
    "    if (arr.rows > 5) println(s\"... ${arr.rows- 5} more rows\")\n",
    "    println(\"-\"*30)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below a formatted output describing the events related to three users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "uid = 33590\n",
      "events count = 355\n",
      "lat,lon = \n",
      "40.735486752   -73.979792352   \n",
      "40.73723724    -73.980824105   \n",
      "40.7424578176  -73.9837482386  \n",
      "40.70736247    -74.008827565   \n",
      "40.737524907   -73.978869923   \n",
      "... 350 more rows\n",
      "------------------------------\n",
      "uid = 139605\n",
      "events count = 4\n",
      "lat,lon = \n",
      "40.7126315333  -74.0444852833  \n",
      "40.6993813387  -74.0393972397  \n",
      "40.69856285    -74.03974055    \n",
      "40.6987988833  -74.0400195     \n",
      "------------------------------\n",
      "uid = 108050\n",
      "events count = 1\n",
      "lat,lon = \n",
      "40.7425115937  -74.0060305595  \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "e_df.take(3).foreach(e => formatUserEvents(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now cluster the events for each user according to the DBSCAN algorithm. This algorithm with cluster those user's events in groups. The rest of the code below reduces those groups to bounding boxes. Next we will use the extracted bounding boxes to score events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import breeze.numerics._\n",
    "import breeze.linalg._\n",
    "\n",
    "def euclideanDistance (a: DenseVector[Double], b: DenseVector[Double]) = norm(a-b, 2)\n",
    "\n",
    "// 1deg at 40deg latitude is 111034.61 meters\n",
    "// set radius at about 200 mt (0.002 * 111034.61)\n",
    "// which is 0.002 in decimal degrees https://en.wikipedia.org/wiki/Decimal_degrees\n",
    "\n",
    "val eps = 0.002\n",
    "val min_points = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nak.cluster._\n",
    "import nak.cluster.GDBSCAN._\n",
    "\n",
    "def dbscan(v : breeze.linalg.DenseMatrix[Double]) = {\n",
    "\n",
    "  val gdbscan = new GDBSCAN(\n",
    "    DBSCAN.getNeighbours(eps, distance=euclideanDistance),\n",
    "    DBSCAN.isCorePoint(min_points)\n",
    "  )\n",
    "\n",
    "  // core DBSCAN algorithm\n",
    "  val clusters = gdbscan cluster v\n",
    "  \n",
    "  // reducing the clusters to bounding boxes\n",
    "  // for simplicity: each user could \n",
    "  clusters.map(\n",
    "    cluster => (\n",
    "      cluster.id.toInt, \n",
    "      cluster.points.size, \n",
    "      cluster.points.map(_.value(0)).min,\n",
    "      cluster.points.map(_.value(1)).min,\n",
    "      cluster.points.map(_.value(0)).max,\n",
    "      cluster.points.map(_.value(1)).max\n",
    "    )\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bboxRdd = e_df.mapValues(dbscan(_)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert back the RDDs to a DataFrame. Now we have a table describing clusters. Each row defines a cluster in terms of user id, cluster id, the number of cluster's events and the bounding box of the cluster. Each user can have multiple clusters, and some users might have no cluster at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|  uid|cid|csize|      lat_min|       lon_min|      lat_max|       lon_max|\n",
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|33590|  1|   12|  40.73723724| -73.980824105| 40.737524907| -73.978869923|\n",
      "|33590|  2|    7|40.7419666616| -73.984329364| 40.742535639| -73.982899202|\n",
      "|33590|  3|    6| 40.705680584| -74.009272538|  40.70736247| -74.008827565|\n",
      "|33590|  5|   70|40.7356195403|-73.9754120581| 40.736689817| -73.973834661|\n",
      "|33590|  7|  102|40.7021876333| -74.011605514|  40.70539715| -74.009128417|\n",
      "|33590| 14|   13| 40.737480983| -73.974163929| 40.737480983| -73.974163929|\n",
      "|33590| 20|   18|  40.73773491| -73.983837394| 40.739129691|  -73.98285782|\n",
      "|33590| 21|   15|40.7371600561|-73.9878805558|   40.7379291|-73.9848974977|\n",
      "|33590| 26|   14| 40.744767148|   -73.9770325|    40.744856| -73.975315143|\n",
      "|33590| 44|    8|40.7355633167|-73.9912224008|40.7377660167|-73.9884488333|\n",
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val bbox_df = bboxRdd.\n",
    "  flatMapValues(x => x).\n",
    "  map(x => (x._1, x._2._1, x._2._2,x._2._3,x._2._4,x._2._5,x._2._6)).\n",
    "  toDF(\"uid\", \"cid\", \"csize\", \"lat_min\", \"lon_min\", \"lat_max\", \"lon_max\").\n",
    "  filter($\"cid\" > 0)\n",
    "\n",
    "bbox_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Events: looking for outliers\n",
    "\n",
    "We will now score events, and look if some of them are located outside the computed clusters' bounding boxes. Firstly, we join the table of events with the table of clusters. Let's filter out users which do not have enough points as those users have no clusters associated with them and there is no sufficient data to determine outliers. In the code above, we need a user to have at least 3 events in a region of 0.1 degrees in order to have a DBSCAN cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+---------------------+----+-------------+--------------+-------------+--------------+-------------+--------------+\n",
      "|ts                   |uid |lat          |lon           |lat_min      |lon_min       |lat_max      |lon_max       |\n",
      "+---------------------+----+-------------+--------------+-------------+--------------+-------------+--------------+\n",
      "|2009-12-06 22:47:17.0|4831|40.7489512339|-73.9955051586|40.7489512339|-73.9955051586|40.750613794 |-73.993434906 |\n",
      "|2009-12-06 22:47:17.0|4831|40.7489512339|-73.9955051586|40.7503537066|-73.99460925  |40.7527147753|-73.9925895262|\n",
      "|2009-12-06 22:47:17.0|4831|40.7489512339|-73.9955051586|40.7482404588|-73.9927482605|40.7503009825|-73.9919929265|\n",
      "|2009-12-06 22:47:03.0|4831|40.7492524   |-73.9951317833|40.7489512339|-73.9955051586|40.750613794 |-73.993434906 |\n",
      "|2009-12-06 22:47:03.0|4831|40.7492524   |-73.9951317833|40.7503537066|-73.99460925  |40.7527147753|-73.9925895262|\n",
      "+---------------------+----+-------------+--------------+-------------+--------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val bbox_events = events.\n",
    "  join(bbox_df, events(\"events.uid\") === bbox_df(\"uid\"), \"full\").\n",
    "  select(\"events.ts\",\"events.uid\",\"lat\",\"lon\",\"lat_min\",\"lon_min\",\"lat_max\",\"lon_max\").\n",
    "  filter($\"lat_min\".isNotNull)\n",
    "\n",
    "bbox_events.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class EventBbox(\n",
    "  ts: Timestamp,\n",
    "  uid: Long, \n",
    "  lat:Double, \n",
    "  lon: Double, \n",
    "  lat_min:Double, \n",
    "  lon_min:Double, \n",
    "  lat_max:Double, \n",
    "  lon_max:Double)\n",
    "  \n",
    "case class EventDetected(\n",
    "  ts: Timestamp,\n",
    "  uid: Long, \n",
    "  lat: Double, \n",
    "  lon: Double, \n",
    "  bbox: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bbox_check( x:EventBbox): Boolean = {\n",
    "  x.lon >= x.lon_min &\n",
    "  x.lon <= x.lon_max &\n",
    "  x.lat >= x.lat_min &\n",
    "  x.lat <= x.lat_max   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the newer Dataset API, which is a DataFrame where row are handled as typed objects. In particular, we are converting the events row into a `EventDetected` object and then we check if the event is within the boundary of the given cluster. Since each user might have more than one cluster, we check each event against all the user's clusters and we consider it an outlier if none of the check is returns a positive outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val scored_events = bbox_events.\n",
    "  as[EventBbox].\n",
    "  map(x => EventDetected(x.ts, x.uid, x.lat, x.lon, bbox_check(x))).\n",
    "  groupBy($\"ts\", $\"uid\").\n",
    "  reduce( (x,y) => if (x.bbox) x else y ).\n",
    "  map(x => x._2).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below the outlier scoring for `uid=4831`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+--------------------+----+-------------+--------------+-----+\n",
      "|                  ts| uid|          lat|           lon| bbox|\n",
      "+--------------------+----+-------------+--------------+-----+\n",
      "|2009-11-16 12:55:...|4831|40.7515061333|-73.9928534833| true|\n",
      "|2009-12-02 13:32:...|4831|40.7504689667|   -73.9987564|false|\n",
      "|2009-12-06 22:44:...|4831|   40.7515816|  -73.99429768| true|\n",
      "|2009-12-06 22:47:...|4831|40.7489512339|-73.9955051586| true|\n",
      "|2009-11-16 12:50:...|4831|40.7524687739|-73.9944867037| true|\n",
      "|2009-11-16 12:54:...|4831|40.7525623527|-73.9944184053| true|\n",
      "|2009-11-16 12:37:...|4831|40.7529089392|-73.9966748478|false|\n",
      "|2009-11-16 12:59:...|4831|40.7473494768|-74.0008061373|false|\n",
      "|2009-12-02 13:32:...|4831|  40.75168975|-73.9941437667| true|\n",
      "|2009-11-16 12:34:...|4831|   40.7518756|  -73.99460925| true|\n",
      "+--------------------+----+-------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_events.\n",
    "  filter(_.uid==4831).\n",
    "  show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see three items are found outside those bounding boxes. Although this is not yet a strong indicator for an anomaly per se, it can constitute a very relevant signal if combined with other signals as seen above. Many improvements can be done to the above core idea, for instance, by including relations and interaction between users and more refined analysis of clusters, using for instance convex hulls instead of bounding boxes and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this notebook, thanks for keeping up with me till here. Best wishes for your data science projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
